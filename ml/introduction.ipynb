{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習入門\n",
    "\n",
    "*機械学習入門*と呼ばれる記事は、おそらくwebサイト上に数千件あります。本稿では、これらの記事を書き直すのではなく、化学での応用に焦点を当てた主要な概念を紹介することにします。 以下に入門的な資料を挙げます。新しくてより良いものがあれば教えてください。\n",
    "\n",
    "1. 原著者が大学院で初めて読んだ、機械学習についてのEthem Alpaydinによって書かれた本{cite}`alpaydin2020introduction`\n",
    "2. Nils Nillsonのオンラインブック [<ins>Introductory Machine Learning</ins>](https://ai.stanford.edu/~nilsson/mlbook.html)\n",
    "3. 物質科学における機械学習についての2つのreview{cite}`fung2021benchmarking,balachandran2019machine`\n",
    "4. 計算科学における機械学習についての2つのreview{cite}`gomez2020machine`\n",
    "5. 金属科学における機械学習についての2つのreview{cite}`nandy2018strategies`\n",
    "\n",
    "これらの資料から、機械学習がデータをモデリングする手法であり、一般的にはには予測機能を持つことを学んでいただけると思います。機械学習には多くの手法が含まれますが、ここでは深層学習を学ぶために必要なものだけを取り上げます。例えば、ランダムフォレスト、サポートベクターマシン、最近傍探索などは広く使われている機械学習手法で、今でも有効な手法ですが、ここでは取り上げません。\n",
    "\n",
    "```{admonition} 読者層と目的\n",
    "本章は、化学とpythonについてある程度知識のある機械学習の初心者を対象としており、そうでない場合には上記の入門記事のいずれかに目を通しておくことをお勧めします。この記事では、`pandas`の知識（カラムの読み込みと選択）、`rdkit`の知識（分子の描き方）、分子を[SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) {cite}`weininger1988smiles`として保存する方法についてある程度の知識を想定しています。この章を読むと、以下のことができるようになると想定されます。\n",
    "\n",
    "  * 特徴量、ラベルの定義\n",
    "  * 教師あり学習と教師なし学習を区別する。\n",
    "  * 損失関数とは何か、勾配降下法を用いてどのように最小化できるかを理解する。\n",
    "  * モデルとは何か、特徴とラベルとの関係を理解する。\n",
    "  * データのクラスタリングができ、それがデータについて何を示すかを説明できる。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ingredients \n",
    "\n",
    "機械学習とは、データに当てはめてモデルを構築することを目指す分野です。\n",
    "まず、言葉を定義します。\n",
    "\n",
    "**特徴量** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;次元$D$の$N$個のベクトル $\\{\\vec{x}_i\\}$ の集合です。実数、整数等が用いられます。\n",
    "\n",
    "**ラベル** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N$ 個の整数または実数の集合 $\\{y_i\\}$。$y_i$ は通常スカラーです。\n",
    "  \n",
    "**ラベル付きデータ** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N$ 個のtupleからなる集合 $\\{\\left(\\vec{x}_i, y_i\\right)\\}$ を指します。\n",
    "\n",
    "**ラベルなしデータ** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;ラベル $y$ が未知の $N$ 個の特徴量 $\\{\\vec{x}_i\\}$ の集合を指します。\n",
    "\n",
    "**モデル**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;特徴ベクトルを受け取り、予測結果 $\\hat{y}$ を出力する関数 $f(\\vec{x})$ を指します。\n",
    "\n",
    "**予測結果**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 与えられた入力 $\\vec{x}$ に対し、モデルを通して得られた予測結果 $\\hat{y}$ のことを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教師あり学習\n",
    "\n",
    "最初のタスクは**教師あり学習**です。教師あり学習とは、データで学習したモデルで $\\vec{x}$ から $y$ を予測する方法です。このタスクは、データセットに含まれるラベルをアルゴリズムに教えることで学習を進めるため、*教師あり*学習と呼ばれています。もう一つの方法は**教師なし学習**で、アルゴリズムにラベルを教えない方法です。この教師あり／教師なしの区別は後でもっと厳密になりますが、今のところはこの定義で十分です。\n",
    "\n",
    "例として、AqSolDB{cite}`Sorkun2019`という、約1万種類の化合物と、その水への溶解度の測定結果(ラベル)についてのデータセットを使ってみます。このデータセットには、機械学習に利用できる分子特性（特徴量）も含まれています。溶解度の測定結果は、化合物の水への溶解度をlog molarityの単位で表したものになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running This Notebook\n",
    "\n",
    "\n",
    "上にある &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; をクリックすると、このページがインタラクティブなGoogle Colab Notebookとして起動されるようになります。 パッケージのインストールについては、以下を参照してください。\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "パッケージをインストールするには、新しいセルで次のコードを実行します。\n",
    "\n",
    "```\n",
    "!pip install dmol-book\n",
    "```\n",
    "\n",
    "インストールに問題が生じた場合は、[このリンク](https://github.com/whitead/dmol-book/blob/master/package/requirements.txt)から使用されているパッケージリストの最新版を入手することができます。\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのロード\n",
    "\n",
    "データをダウンロードし、[Pandas](https://pandas.pydata.org/)のデータフレームにロードします。以下のセルでは、インポートや必要なパッケージのインストールを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.example_libraries import optimizers\n",
    "import sklearn.manifold, sklearn.cluster\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.Draw\n",
    "import dmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soldata = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&gbrecs=true')\n",
    "# had to rehost because dataverse isn't reliable\n",
    "soldata = pd.read_csv(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/curated-solubility-dataset.csv\"\n",
    ")\n",
    "soldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ探索\n",
    "\n",
    "```{margin} EDA\n",
    "EDAを特徴量の選択として行う場合、テストデータでモデルの選択に影響を与えないように、EDAを行う前にtrain/test/(valid)の分割を行う必要があります。\n",
    "```\n",
    "\n",
    "分子には、分子量、回転可能な結合、価電子など、様々な特徴量となりうるものがあります。そしてもちろん、今回のデータセットにおいてラベルとなる**溶解度**という指標もあります。このデータセットに対して私たちが常に最初に行うべきことの一つは、**探索的データ解析**（EDA）と呼ばれるプロセスでデータについての理解を深めることです。まず、ラベルやデータの大枠を知るために、いくつかの具体的な例を調べることから始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one molecule\n",
    "mol = rdkit.Chem.MolFromInchi(soldata.InChI[0])\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはデータセットのうち、最初の分子を[rdkit](https://rdkit.org/)を使ってレンダリングしたものです。\n",
    "\n",
    "それでは、溶解度データの**範囲**とそれを構成する分子についてなんとなく理解するために、極値を見てみましょう。まず、溶解度の確率分布の形と極値を知るために、({obj}`seaborn.distplot` を使って)溶解度のヒストグラムを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(soldata.Solubility)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図では、溶解度のヒストグラムとカーネル密度推定値を重ね合わせています。このヒストグラムから、溶解度は約-13から2.5まで変化し、正規分布していないことがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 3 lowest and 3 highest solubilities\n",
    "soldata_sorted = soldata.sort_values(\"Solubility\")\n",
    "extremes = pd.concat([soldata_sorted[:3], soldata_sorted[-3:]])\n",
    "\n",
    "# We need to have a list of strings for legends\n",
    "legend_text = [\n",
    "    f\"{x.ID}: solubility = {x.Solubility:.2f}\" for x in extremes.itertuples()\n",
    "]\n",
    "\n",
    "# now plot them on a grid\n",
    "extreme_mols = [rdkit.Chem.MolFromInchi(inchi) for inchi in extremes.InChI]\n",
    "rdkit.Chem.Draw.MolsToGridImage(\n",
    "    extreme_mols, molsPerRow=3, subImgSize=(250, 250), legends=legend_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "極端な分子の例では、高塩素化合物が最も溶解度が低く、イオン性化合物が溶解度が高いことがわかります。A-2918は**外れ値**、つまり間違いなのでしょうか？また、NH$_3$ は本当にこれらの有機化合物に匹敵するのでしょうか？このような疑問は、モデリングを行う*前に*検討すべきことです。\n",
    "\n",
    "```{margin} 外れ値\n",
    "\n",
    "外れ値とは、正規のデータ分布から外れた極端な値のことです。間違いであったり、異なる分布であったりします。（例えば、有機分子ではなく金属であるなど）外れ値はモデル学習に強い影響を与える可能性があります。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の相関\n",
    "次に、特徴量と溶解度(ラベル)の相関を調べてみましょう。 `SD` (標準偏差)、`Ocurrences` (その分子が構成するデータベースで何回出現したか)、`Group` (データの出所) など、特徴量や溶解度とは関係のないカラムがいくつかあることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_start_at = list(soldata.columns).index(\"MolWt\")\n",
    "feature_names = soldata.columns[features_start_at:]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=5, ncols=4, sharey=True, figsize=(12, 8), dpi=300)\n",
    "axs = axs.flatten()  # so we don't have to slice by row and column\n",
    "for i, n in enumerate(feature_names):\n",
    "    ax = axs[i]\n",
    "    ax.scatter(\n",
    "        soldata[n], soldata.Solubility, s=6, alpha=0.4, color=f\"C{i}\"\n",
    "    )  # add some color\n",
    "    if i % 4 == 0:\n",
    "        ax.set_ylabel(\"Solubility\")\n",
    "    ax.set_xlabel(n)\n",
    "# hide empty subplots\n",
    "for i in range(len(feature_names), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分子量や水素結合の数は、少なくともこのプロットからは、ほとんど相関がないように見えるのは興味深いことです。MolLogPは溶解性に関連する計算から導出された記述子で、よい相関を持っています。また、これらの特徴量のいくつかは、**分散**が低く、特徴の値が多くのデータに対してほとんど変化しないか、全く変化しないことがわかります（例えば、「NumHDonors」など）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形モデル\n",
    "\n",
    "まず、最も単純なアプローチの1つである線形モデルから始めましょう。これは教師あり学習の最初の例で、これから説明する特徴量の選択が難しいため、ほとんど使われることはありません。\n",
    "\n",
    "\n",
    "```{margin} Autodiff\n",
    "[Autodiff](https://en.wikipedia.org/wiki/Automatic_differentiation) は2つの変数に関する分析的な勾配を計算することができるツールです。\n",
    "```\n",
    "\n",
    "この線形モデルは以下の方程式で定義されます。\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\vec{w} \\cdot \\vec{x} + b\n",
    "\\end{equation}\n",
    "\n",
    "この式は1つのデータ点に対して定義されます。1つの特徴ベクトル $\\vec{x}$ の形状は、(17個の特徴があるため)今回の場合17です。$\\vec{w}$ は長さ17の調整可能なパラメータのベクトルで、 $b$ は調整可能なスカラーです(**バイアス** と呼ばれます)。\n",
    "\n",
    "このモデルは、[``jax``](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)というライブラリを用いて実装します。このライブラリは、autodiffによって解析的勾配を簡単に計算できることを除けば、numpyに非常によく似ています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, w, b):\n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "\n",
    "# test it out\n",
    "x = np.array([1, 0, 2.5])\n",
    "w = np.array([0.2, -0.5, 0.4])\n",
    "b = 4.3\n",
    "\n",
    "linear_model(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} 損失\n",
    "損失とは、モデルの予測値 $\\hat{y}$ とラベル $y$ を受け取り、モデルがどれだけ適合しているかを表す関数から出力されるスカラーです。今回の目標は、この損失を最小化することです。\n",
    "```\n",
    "\n",
    "ここで重要な問題が出てきます。 *調整可能なパラメータ $\\vec{w}$ と $b$ はどのように見つけるのでしょうか？* 線形回帰の古典的な方法では、 $\\vec{w} = (X^TX)^{-1}X^{T}\\vec{y}$ という擬似逆行列を使って調整パラメータを直接計算します。詳しくは [こちら](https://nbviewer.jupyter.org/github/whitead/numerical_stats/blob/master/unit_12/lectures/lecture_1.ipynb#Extending-Least-Squares-to-Multiple-Dimensions-in-Domain---OLS-ND)に詳しく書いてあります。 しかし、今回は、深層学習で行うことを考慮した**反復的**なアプローチを使用します。これは線形回帰の正しい計算方法ではありませんが、深層学習ではよく見る方法なので、反復的な計算方法に慣れるのに便利でしょう。\n",
    "\n",
    "調整可能なパラメータを繰り返し見つけるために、**損失**関数を選び、**勾配**で最小化することにします。これらの量を定義し、いくつかの初期値wとbを用いて損失を計算していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into features, labels\n",
    "features = soldata.loc[:, feature_names].values\n",
    "labels = soldata.Solubility.values\n",
    "\n",
    "feature_dim = features.shape[1]\n",
    "\n",
    "# initialize our paramaters\n",
    "w = np.random.normal(size=feature_dim)\n",
    "b = 0.0\n",
    "\n",
    "# define loss\n",
    "def loss(y, labels):\n",
    "    return jnp.mean((y - labels) ** 2)\n",
    "\n",
    "\n",
    "# test it out\n",
    "y = linear_model(features, w, b)\n",
    "loss(y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "溶解度が-13から2であることを考えると、この損失はひどいものです。しかし、この結果はあくまで初期パラメータから予測しただけなので、正しいのです。\n",
    "\n",
    "\n",
    "### 勾配降下法\n",
    "\n",
    "ここで、調整可能なパラメータに対して損失がどのように変化するかという情報を使って、損失を減らしていきます。\n",
    "今回用いる損失関数を以下のように定義します。:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_i^N \\left[y_i - f(\\vec{x}_i, \\vec{w}, b)\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "この損失関数は**平均2乗誤差**と呼ばれ、しばしばMSEと略されます。調整可能なパラメータに対する損失の勾配を計算することができます。\n",
    "\n",
    "```{margin} jax.grad\n",
    "[jax.grad](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)はPython関数の解析的導関数を計算します。\n",
    "これは2つの引数を取ります。関数と、どの引数に対して微分を行うかについてです。\n",
    "例えば、`f(x, y, z)`について考えると、`jax.grad(f,(1,2))` は $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}$ を与えます。 $x$ はテンソルでも良いことに注意してください。 \n",
    "```\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L}{\\partial w_i}, \\frac{\\partial L}{\\partial b}\n",
    "\\end{equation}\n",
    "\n",
    "ここで、$w_i$ は重みベクトル $i$ 番目の要素である $\\vec{w}$ です。負の勾配の方向に微小量の変化を生むことで、損失を減らすことができます。:\n",
    "\n",
    "\\begin{equation}\n",
    "    (w_i, b') = \\left(w_i - \\eta \\frac{\\partial L}{\\partial w_i}, b - \\eta\\frac{\\partial L}{\\partial b}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "ここで、 $\\eta$ は**学習率**であり、調整可能であるが、学習しないパラメータ（**ハイパーパラメータ**と呼ばれます）です。この例では $1\\times10^{-6}$ と設定しています。一般的には10の累乗で表され、最大でも0.1程度になるように設定されます。それ以上の値は安定性に問題があることが知られています。それでは、**gradient descent**と呼ばれるこの手順を実装してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradients\n",
    "def loss_wrapper(w, b, data):\n",
    "    features = data[0]\n",
    "    labels = data[1]\n",
    "    y = linear_model(features, w, b)\n",
    "    return loss(y, labels)\n",
    "\n",
    "\n",
    "loss_grad = jax.grad(loss_wrapper, (0, 1))\n",
    "\n",
    "# test it out\n",
    "loss_grad(w, b, (features, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配の計算を行ったので、それを数ステップかけて最小化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_progress = []\n",
    "eta = 1e-6\n",
    "data = (features, labels)\n",
    "for i in range(10):\n",
    "    grad = loss_grad(w, b, data)\n",
    "    w -= eta * grad[0]\n",
    "    b -= eta * grad[1]\n",
    "    loss_progress.append(loss_wrapper(w, b, data))\n",
    "plt.plot(loss_progress)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Full Dataset Training Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習曲線\n",
    "\n",
    "上の図は **学習曲線** と呼ばれるものです。これは損失が減少しているかどうかを示しており、モデルが学習を行っていることを表しています。学習曲線は **Learning Curve** とも呼ばれます。X軸はサンプル数、データセットの総反復回数（エポックと呼ばれる）、モデルの学習に使用されたデータ量の他の指標が用いられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ処理\n",
    "\n",
    "```{margin} バッチ\n",
    "バッチはデータのサイズに応じたデータの部分集合のことです。バッチサイズは通常2の累乗で定義されます。（例：4、16、128）。\n",
    "ランダムなバッチを用いることで、勾配降下法は確率的勾配降下法になります。\n",
    "```\n",
    "\n",
    "勾配降下法によって学習が良い感じに進んでいることがわかります。しかし、ちょっとした変更で学習のスピードアップを図ることができます。機械学習で実際に行われている学習方法である **バッチ処理** を使ってみましょう。ちょっとした変更点とは、すべてのデータを一度に使うのではなく、その部分集合の小さな**バッチ**データだけを取るということです。バッチ処理には2つの利点があります。1つはパラメータの更新を計算する時間を短縮できること、もう1つは学習過程をランダムにできることです。このランダム性により、学習の進行を止める可能性のあるる局所的な極小値から逃れることができます。このバッチ処理の追加により、この勾配降下法アルゴリズムは**確率的**となり、**確率的勾配降下法**（SGD）と呼ばれる手法になります。SGDとそのバリエーションは、深層学習における最も一般的な学習方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our paramaters\n",
    "# to be fair to previous method\n",
    "w = np.random.normal(size=feature_dim)\n",
    "b = 0.0\n",
    "\n",
    "loss_progress = []\n",
    "eta = 1e-6\n",
    "batch_size = 32\n",
    "N = len(labels)  # number of data points\n",
    "data = (features, labels)\n",
    "# compute how much data fits nicely into a batch\n",
    "# and drop extra data\n",
    "new_N = len(labels) // batch_size * batch_size\n",
    "\n",
    "# the -1 means that numpy will compute\n",
    "# what that dimension should be\n",
    "batched_features = features[:new_N].reshape((-1, batch_size, feature_dim))\n",
    "batched_labels = labels[:new_N].reshape((-1, batch_size))\n",
    "# to make it random, we'll iterate over the batches randomly\n",
    "indices = np.arange(new_N // batch_size)\n",
    "np.random.shuffle(indices)\n",
    "for i in indices:\n",
    "    # choose a random set of\n",
    "    # indices to slice our data\n",
    "    grad = loss_grad(w, b, (batched_features[i], batched_labels[i]))\n",
    "    w -= eta * grad[0]\n",
    "    b -= eta * grad[1]\n",
    "    # we still compute loss on whole dataset, but not every step\n",
    "    if i % 10 == 0:\n",
    "        loss_progress.append(loss_wrapper(w, b, data))\n",
    "\n",
    "plt.plot(np.arange(len(loss_progress)) * 10, loss_progress)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Batched Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで注目すべき点は、以下の3つです。\n",
    "\n",
    "1. バッチ処理を行わない場合に比べ、損失が小さくなっています。\n",
    "2. データセットを10回で反復するのをやめ、1回だけ反復しているにも関わらず、ステップ数が増えています。\n",
    "3. 損失は常に減少するわけではありません。\n",
    "\n",
    "損失が小さくなる理由は、各データポイントを1回しか見ていないにもかかわらず、より多くのステップを踏むことができるからです。バッチ処理を行うと、バッチごとに勾配降下法の更新を行うため、データセットに対して1回の反復でより多くの更新を行うことができます。具体的には $B$ をバッチサイズとすると、元の勾配降下法では1回しか更新できない所、バッチ処理を行った場合は $N / B$ 回の更新を行うことができます。損失が常に減少しない理由は、評価するたびに異なるデータセットであるためです。データセットからランダムに選定したバッチでは、ある分子が他の分子より予測が難しかったり、1つのバッチに基づいてパラメータを更新しただけなので、各ステップが正しく損失を最小化できているとは限らなかったりします。しかし、バッチがランダムに選定されていると仮定すれば、常に（平均的に）損失の減少量の期待値を向上させることができます。(つまり、損失の期待値を最小化できます)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の標準化\n",
    "\n",
    "It seems we cannot get past a certain loss. If you examine the gradients you'll see some of them are very large and some are very small. Each of the features have different magnitudes. For example, molecular weights are large numbers. The number of rings in a molecule is a small number. Each of these must use the same learning rate, $\\eta$, and that is ok for some but too small for others. If we increase $\\eta$, our training procedure will explode because of these larger feature gradients. A standard trick we can do is make all the features have the same magnitude, using the equation for standardization you might see in your statistics textbook:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{ij} = \\frac{x_{ij} - \\bar{x_j}}{\\sigma_{x_j}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x_j}$ is column mean and $\\sigma_{x_j}$ is column standard deviation. To be careful about contaminating training data with test data -- leaking information between train and test data -- we should only use training data in computing the mean and standard deviation. We want our test data to approximate how we'll use our model on unseen data, so we cannot know what these unseen features means/standard deviations might be and thus cannot use them at training time for standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstd = np.std(features, axis=0)\n",
    "fmean = np.mean(features, axis=0)\n",
    "std_features = (features - fmean) / fstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our paramaters\n",
    "# since we're changing the features\n",
    "w = np.random.normal(scale=0.1, size=feature_dim)\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "loss_progress = []\n",
    "eta = 1e-2\n",
    "batch_size = 32\n",
    "N = len(labels)  # number of data points\n",
    "data = (std_features, labels)\n",
    "# compute how much data fits nicely into a batch\n",
    "# and drop extra data\n",
    "new_N = len(labels) // batch_size * batch_size\n",
    "num_epochs = 3\n",
    "\n",
    "# the -1 means that numpy will compute\n",
    "# what that dimension should be\n",
    "batched_features = std_features[:new_N].reshape((-1, batch_size, feature_dim))\n",
    "batched_labels = labels[:new_N].reshape((-1, batch_size))\n",
    "indices = np.arange(new_N // batch_size)\n",
    "\n",
    "# iterate through the dataset 3 times\n",
    "for epoch in range(num_epochs):\n",
    "    # to make it random, we'll iterate over the batches randomly\n",
    "    np.random.shuffle(indices)\n",
    "    for i in indices:\n",
    "        # choose a random set of\n",
    "        # indices to slice our data\n",
    "        grad = loss_grad(w, b, (batched_features[i], batched_labels[i]))\n",
    "        w -= eta * grad[0]\n",
    "        b -= eta * grad[1]\n",
    "        # we still compute loss on whole dataset, but not every step\n",
    "        if i % 50 == 0:\n",
    "            loss_progress.append(loss_wrapper(w, b, data))\n",
    "\n",
    "plt.plot(np.arange(len(loss_progress)) * 50, loss_progress)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we safely increased our learning rate to 0.01, which is possible because all the features are of similar magnitude. We also could keep training, since we're gaining improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model Performance\n",
    "\n",
    "This is a large topic that we'll explore more, but the first thing we typically examine in supervised learning is a **parity plot**, which shows our predictions vs. our label prediction. What's nice about this plot is that it works no matter what the dimensions of the features are. A perfect fit would fall onto the line at $y = \\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = linear_model(std_features, w, b)\n",
    "\n",
    "plt.plot([-100, 100], [-100, 100])\n",
    "plt.scatter(labels, predicted_labels, s=4, alpha=0.7)\n",
    "plt.xlabel(\"Measured Solubility $y$\")\n",
    "plt.ylabel(\"Predicted Solubility $\\hat{y}$\")\n",
    "plt.xlim(-13.5, 2)\n",
    "plt.ylim(-13.5, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final model assessment can be done with loss, but typically other metrics are also used. In regression, a **correlation coefficient** is typically reported in addition to loss. In our example, this is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice correlation between predict/labels\n",
    "# from correlation matrix\n",
    "np.corrcoef(labels, predicted_labels)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS USED TO GENERATE A FIGURE\n",
    "# AND NOT RELATED TO CHAPTER\n",
    "# YOU CAN SKIP IT\n",
    "from myst_nb import glue\n",
    "\n",
    "glue(\"corr\", np.round(np.corrcoef(labels, predicted_labels)[0, 1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation coefficient of {glue:}`corr` is OK, but not great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "In unsupervised learning, the goal is to predict $\\hat{y}$ *without* labels. This seems like an impossible task. How do we judge success? Typically, unsupervised learning can be broken into three categories:\n",
    "\n",
    "**Clustering**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Here we assume $\\{y_i\\}$ is a class variable and try to partition our features into these classes. In clustering we are simultaneously learning the definition of the classes (called clusters) and which cluster each feature should be assigned to.\n",
    "\n",
    "```{margin} Class\n",
    "In machine learning, a class is a type of label like ``dog`` or ``cat``. Formally,\n",
    "we have a set of possible labels (e.g., all animals) and each feature vector has one (hard) or a \n",
    "probability distribution of classes (soft).\n",
    "```\n",
    "\n",
    "**Finding Signal**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $x$ is assumed to be made of two components: noise and signal ($y$). We try to separate the signal $y$ from $x$ and discard noise. Highly-related with **representation learning**, which we'll see later.\n",
    "\n",
    "\n",
    "**Generative**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Generative methods are methods where we try to learn $P(\\vec{x})$ so that we can sample new values of $\\vec{x}$. It is analogous to $y$ being probability and we're trying to estimate it. We'll see these more later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering is historically one of the most well-known and still popular machine learning methods. It's always popular because it can provide new insight from data. Clustering gives class labels where none existed and thus can help find patterns in data. This is also a reason that it has become less popular in chemistry (and most fields): there is no right or wrong answer. Two people doing clustering independently will often arrive at different answers. Nevertheless, it should be a tool you know and can be a good exploration strategy.\n",
    "\n",
    "```{margin} cluster labels\n",
    "Clustering comes in many variants and some blur what exactly $y_i$ is. For example, in some clustering methods $y_i$ can include no assignment or $y_i$ is not a single class, but a tree of classes.\n",
    "```\n",
    "\n",
    "We'll look at the classic clustering method: k-means. Wikipedia has a [great article](https://en.wikipedia.org/wiki/K-means_clustering) on this classic algorithm, so I won't try to repeat that. To make our clustering actually visible, we'll start by projecting our features into 2 dimensions. This will be covered in representation learning, so don't worry about these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get down to 2 dimensions for easy visuals\n",
    "embedding = sklearn.manifold.Isomap(n_components=2)\n",
    "# only fit to every 25th point to make it fast\n",
    "embedding.fit(std_features[::25, :])\n",
    "reduced_features = embedding.transform(std_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to zoom into the middle 99th percentile of the data since some of the points are extremely far away (though that is interesting!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow, xhi = np.quantile(reduced_features, [0.005, 0.995], axis=0)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.scatter(\n",
    "    reduced_features[:, 0],\n",
    "    reduced_features[:, 1],\n",
    "    s=4,\n",
    "    alpha=0.7,\n",
    "    c=labels,\n",
    "    edgecolors=\"none\",\n",
    ")\n",
    "plt.xlim(xlow[0], xhi[0])\n",
    "plt.ylim(xlow[1], xhi[1])\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"Solubility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} Dimensionality Reduction\n",
    "Reducing $\\vec{x}$, your feature vectors to a low\n",
    "dimensional space. The classic example is PCA, which is a \n",
    "linear operator. However, most prefer nonlinear methods \n",
    "like [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The dimensionality reduction has made our features only 2 dimensions. We can see some structure, especially with the solubility as the coloring. Note in these kind of plots, where we have reduced dimensions in someway, we do not label the axes because they are arbitrary.\n",
    "\n",
    "Now we cluster. The main challenge in clustering is deciding how many clusters there should be. There are a number of methods out there, but they basically come down to intuition. You, as the chemist, should use some knowledge outside of the data to intuit what is the cluster number. Sounds unscientific? Yeah, that's why clustering is hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# cluster - using whole features\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(std_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simple procedure! Now we'll visualize by coloring our data by the class assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "point_colors = [f\"C{i}\" for i in kmeans.labels_]\n",
    "plt.scatter(\n",
    "    reduced_features[:, 0],\n",
    "    reduced_features[:, 1],\n",
    "    s=4,\n",
    "    alpha=0.7,\n",
    "    c=point_colors,\n",
    "    edgecolors=\"none\",\n",
    ")\n",
    "# make legend\n",
    "legend_elements = [\n",
    "    plt.matplotlib.patches.Patch(\n",
    "        facecolor=f\"C{i}\", edgecolor=\"none\", label=f\"Class {i}\"\n",
    "    )\n",
    "    for i in range(4)\n",
    "]\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.xlim(xlow[0], xhi[0])\n",
    "plt.ylim(xlow[1], xhi[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Cluster Number\n",
    "\n",
    "How do we know we had the correct number? Intuition. There is one tool we can use to help us, called an **elbow plot**. The k-means clusters can be used to compute the mean squared distance from cluster center, basically a version of loss function. However, if we treat cluster number as a trainable parameter we'd find the best fit at the cluster number being equal to number of data points. Not helpful! However, we can see when the slope of this loss becomes approximately constant and assume that those extra clusters are adding no new insight. Let's plot the loss and see what happens. Note we'll be using a subsample of the dataset to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an elbow plot\n",
    "loss = []\n",
    "cn = range(2, 15)\n",
    "for i in cn:\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=i, random_state=0)\n",
    "    # use every 50th point\n",
    "    kmeans.fit(std_features[::50])\n",
    "    # we get score -> opposite of loss\n",
    "    # so take -\n",
    "    loss.append(-kmeans.score(std_features[::50]))\n",
    "\n",
    "plt.plot(cn, loss, \"o-\")\n",
    "plt.xlabel(\"Cluster Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Elbow Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is the transition? If I squint, maybe at 6? 3? 4? 7? Let's choose 4 because it sounds nice and is plausible based on the data. The last task is to get some insight into what the clusters actually are. We can extract the most centered data points (closest to cluster center) and consider them to be representative of the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "alt": "Grid of rendered molecular structures that are representative cluster centers"
   },
   "outputs": [],
   "source": [
    "# cluster - using whole features\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(std_features)\n",
    "\n",
    "cluster_center_idx = []\n",
    "for c in kmeans.cluster_centers_:\n",
    "    # find point closest\n",
    "    i = np.argmin(np.sum((std_features - c) ** 2, axis=1))\n",
    "    cluster_center_idx.append(i)\n",
    "cluster_centers = soldata.iloc[cluster_center_idx, :]\n",
    "\n",
    "legend_text = [f\"Class {i}\" for i in range(4)]\n",
    "\n",
    "# now plot them on a grid\n",
    "cluster_mols = [rdkit.Chem.MolFromInchi(inchi) for inchi in cluster_centers.InChI]\n",
    "rdkit.Chem.Draw.MolsToGridImage(\n",
    "    cluster_mols, molsPerRow=2, subImgSize=(400, 400), legends=legend_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly are these classes? Unclear. We intentionally did not reveal solubility (unsupervised learning) so there is not necessarily any connection with solubility. These classes are more a result of which features were chosen for the dataset. You could make a hypothesis, like class 1 is all negatively charged or class 0 is aliphatic, and investigate. Ultimately though there is no *best* clustering and often unsupervised learning is more about finding insight or patterns and not about producing a highly-accurate model.\n",
    "\n",
    "The elbow plot method is one of many approaches to selecting cluster number {cite}`pham2005selection`. I prefer it because it's quite clear that you are using intuition. More sophisticated methods sort-of conceal the fact that there is no right or wrong answer in clustering. \n",
    "\n",
    "\n",
    "\n",
    "```{note}\n",
    "This process does not result in a function that predicts solubility. We might try to gain insight about predicting solubility with our predicted classes, but that is not the goal of clustering.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "* Supervised machine learning is building models that can predict labels $y$ from input features $\\vec{x}$.\n",
    "* Data can be labeled or unlabeled. \n",
    "* Models are trained by minimizing loss with stochastic gradient descent.\n",
    "* Unsupervised learning is building models that can find patterns in data.\n",
    "* Clustering is unsupervised learning where the model groups the data points into clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Data\n",
    "\n",
    "1. Using `numpy` reductions `np.amin`, `np.std`, etc. (not `pandas`!), compute the mean, min, max, and standard deviation for each feature across all data points. \n",
    "\n",
    "2. Use rdkit to draw the 2 highest molecular weight molecules. Note they look strange. \n",
    "\n",
    "### Linear Models\n",
    "\n",
    "1. Prove that a nonlinear model like $y = \\vec{w_1} \\cdot \\sin\\left(\\vec{x}\\right) + \\vec{w_2} \\cdot \\vec{x} + b$ could be represented as a linear model.\n",
    "\n",
    "2. Write out the linear model equation in Einstein notation in batched form. **Batched form** means we explicitly have an index indicating batch. For example, the labels will be $y_{bi}$  where $b$ indicates the batch of data and $i$ indicates the individual data point.\n",
    "\n",
    "\n",
    "### Minimizing Loss\n",
    "\n",
    "1. We standardized the features, but not the labels. Would standardizing the labels affect our choice of learning rate? Prove your answer.\n",
    "\n",
    "2. Implement a loss that is mean absolute error, instead of mean squared error. Compute its gradient using `jax`.\n",
    "\n",
    "2. Using the standardized features, show what effect batch size has on training. Use batch sizes of 1, 8, 32, 256, 1024. Make sure you re-initialize your weights in between each run. Plot the log-loss for each batch size on the same plot. Describe your results.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "1. We say that clustering is a type of unsupervised learning and that it predicts the labels. What exactly are the predicted labels in clustering? Write down what the predicted labels might look like for a few data points.\n",
    "\n",
    "2. In clustering, we predict labels from features. You can still cluster if you have labels, by just pretending they are features. Give two reasons why it would not be a good idea to do clustering in this manner, where we treat the labels as features and try to predict new labels that represent class.\n",
    "\n",
    "3. On the isomap plot (reduced dimension plot), color the points by which group they fall in (G1, G2, etc.). Is there any relationship between this and the clustering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('3.8.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c954abbd500635c929c825ee4f107e63a7c5cb2bc239f8f781b23be55e7a62ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
